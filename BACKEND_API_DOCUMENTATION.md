# Genius AI - Backend & API Documentation

## Technology Stack

### Backend Framework
- **FastAPI** - Modern, high-performance Python web framework
- **Uvicorn** - Lightning-fast ASGI server
- **Python 3.10+** - Latest Python features

### AI & ML Libraries
- **Groq SDK** - For accessing Groq's AI models (FREE!)
- **Pillow (PIL)** - Image processing and metadata extraction
- **OpenAI SDK** (Optional) - For GPT-4 Vision support

### Core Libraries
- **Pydantic** - Data validation and settings management
- **python-multipart** - File upload handling
- **CORS Middleware** - Cross-origin resource sharing

---

## AI Models Used

### Primary Text Model
**Llama 3.3 70B Versatile**
- Provider: Groq (FREE)
- Parameters: 70 billion
- Context: 8,192 tokens
- Use: General chat, reasoning, analysis
- Temperature: 0.8 (creative)
- Max Tokens: 8,192 (detailed responses)

### Alternative Models Available
1. **Llama 3.1 8B Instant** - Super fast responses
2. **Mixtral 8x7B** - Long context (32k tokens)
3. **Gemma 2 9B** - Balanced performance

### Vision Capability
**Method: Intelligent Metadata Analysis + 70B Reasoning**
- Extracts: Dimensions, format, file size, colors, transparency
- Uses: Llama 3.3 70B to reason about image content
- Fallback: GPT-4 Vision (if API key provided)

---

## API Endpoints

### Base URL
```
http://localhost:8000
```

### Interactive Documentation
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

---

## Endpoint Details

### 1. GET `/`
**Root endpoint - Server info**

**Response:**
```json
{
  "status": "running",
  "version": "4.0",
  "features": [
    "GPT-4 Vision support (if API key provided)",
    "Intelligent metadata-based analysis",
    "Technical image analysis",
    "70B AI reasoning about images",
    "Comprehensive fallback system"
  ],
  "gpt4_vision_available": false
}
```

**Example:**
```bash
curl http://localhost:8000/
```

---

### 2. GET `/health`
**Health check endpoint**

**Response:**
```json
{
  "status": "healthy",
  "gpt4_vision": false,
  "intelligent_fallback": true
}
```

**Example:**
```bash
curl http://localhost:8000/health
```

---

### 3. POST `/chat`
**Main chat endpoint - Text & Image analysis**

**Request Body:**
```json
{
  "message": "Your question or prompt",
  "model": "llama-3.3-70b-versatile",
  "temperature": 0.8,
  "max_tokens": 8192,
  "system_prompt": "Optional custom system prompt",
  "image_data": "data:image/png;base64,..." // Optional
}
```

**Parameters:**
- `message` (required): User's input text
- `model` (optional): AI model to use (default: llama-3.3-70b-versatile)
- `temperature` (optional): 0.0-1.0, higher = more creative (default: 0.8)
- `max_tokens` (optional): Maximum response length (default: 8192)
- `system_prompt` (optional): Custom instructions for AI
- `image_data` (optional): Base64 encoded image for analysis

**Response:**
```json
{
  "response": "AI's detailed response...",
  "model": "Llama 3.3 70B",
  "tokens_used": 1234,
  "analyzed_file": null,
  "processing_time": 2.45,
  "vision_method": "intelligent-metadata" // Only for images
}
```

**Example - Text Chat:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explain quantum computing in simple terms"
  }'
```

**Example - Image Analysis:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Analyze this image",
    "image_data": "data:image/png;base64,iVBORw0KGgoAAAANS..."
  }'
```

**Example - Custom Model:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Analyze this long document...",
    "model": "mixtral-8x7b-32768"
  }'
```

---

### 4. POST `/upload`
**File upload endpoint - Images & Documents**

**Request:** Multipart form data

**Form Fields:**
- `file` (required): File to upload (max 20MB)
- `message` (optional): Question/prompt about the file (default: "Analyze this file in detail")
- `model` (optional): AI model to use (default: llama-3.3-70b-versatile)

**Supported File Types:**
- **Images**: PNG, JPG, JPEG, GIF, WebP, BMP
- **Documents**: TXT, PDF, MD (Markdown)

**Response:**
```json
{
  "response": "Detailed analysis of the file...",
  "model": "Llama 3.3 70B (Intelligent Analysis)",
  "tokens_used": null,
  "analyzed_file": "image.png",
  "processing_time": 3.12,
  "vision_method": "intelligent-metadata"
}
```

**Example - Upload Image:**
```bash
curl -X POST http://localhost:8000/upload \
  -F "file=@photo.jpg" \
  -F "message=What objects are in this image?"
```

**Example - Upload Document:**
```bash
curl -X POST http://localhost:8000/upload \
  -F "file=@document.pdf" \
  -F "message=Summarize this document"
```

---

## Request/Response Models

### ChatRequest (POST /chat)
```python
class ChatRequest(BaseModel):
    message: str                                    # Required
    model: Optional[str] = "llama-3.3-70b-versatile"  # Optional
    temperature: Optional[float] = 0.8              # Optional (0.0-1.0)
    max_tokens: Optional[int] = 8192                # Optional (1-8192)
    system_prompt: Optional[str] = None             # Optional
    image_data: Optional[str] = None                # Optional (Base64)
```

### ChatResponse
```python
class ChatResponse(BaseModel):
    response: str                      # AI's response text
    model: str                         # Model name used
    tokens_used: Optional[int]         # Token count (if available)
    analyzed_file: Optional[str]       # Filename (if file uploaded)
    processing_time: Optional[float]   # Time in seconds
    vision_method: Optional[str]       # Vision analysis method used
```

---

## Rate Limiting

**Limits:**
- 60 requests per minute per IP address
- Resets every 60 seconds

**Error Response (429):**
```json
{
  "detail": "Rate limit exceeded. Maximum 60 requests per minute."
}
```

**Exempt Endpoints:**
- `GET /`
- `GET /health`

---

## Image Analysis

### How It Works

1. **Extract Metadata** (using Pillow):
   - Format (PNG, JPG, etc.)
   - Dimensions (width x height)
   - File size
   - Color mode (RGB, RGBA, Grayscale)
   - Transparency
   - Number of colors

2. **AI Reasoning** (using Llama 3.3 70B):
   - Analyzes technical specifications
   - Infers likely content based on dimensions
   - Provides detailed insights
   - Suggests use cases

3. **Fallback Options**:
   - Primary: Intelligent metadata analysis (FREE)
   - Secondary: GPT-4 Vision (requires OpenAI API key)

### What You Get

**For a 1920x1080 PNG image:**
```
Image Type: Likely a screenshot or presentation slide (16:9 aspect ratio)
Resolution: High quality (Full HD)
File Size Analysis: 463.4 KB suggests moderate compression
Color Analysis: RGB color mode with no transparency
Content Inference: Based on standard 16:9 ratio, likely contains:
  - Desktop screenshot
  - Video thumbnail
  - Presentation slide
  - Web page capture
Recommendations: Suitable for web use, presentations, or documentation
```

---

## Error Handling

### Error Responses

**400 Bad Request:**
```json
{
  "detail": "Unsupported file type: application/zip"
}
```

**413 Payload Too Large:**
```json
{
  "detail": "File too large. Maximum size is 20MB."
}
```

**429 Too Many Requests:**
```json
{
  "detail": "Rate limit exceeded. Maximum 60 requests per minute."
}
```

**500 Internal Server Error:**
```json
{
  "detail": "Error message with details"
}
```

---

## Environment Variables

### Required
- `GROQ_API_KEY` - Already configured in code

### Optional
- `OPENAI_API_KEY` - For GPT-4 Vision support (premium)

**To add OpenAI (optional):**
```bash
# In .env file
OPENAI_API_KEY=sk-...your-key-here...

# Or in code
OPENAI_API_KEY = "sk-...your-key-here..."
```

---

## Frontend Integration

### Example - React/TypeScript

```typescript
// Chat with text
const response = await fetch('http://localhost:8000/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: userInput,
    model: 'llama-3.3-70b-versatile',
    temperature: 0.8
  })
})

const data = await response.json()
console.log(data.response) // AI response
```

```typescript
// Upload image
const formData = new FormData()
formData.append('file', imageFile)
formData.append('message', 'Analyze this image')

const response = await fetch('http://localhost:8000/upload', {
  method: 'POST',
  body: formData
})

const data = await response.json()
console.log(data.response) // Image analysis
```

---

## Performance

### Response Times
- **Text Chat**: 1-3 seconds
- **Image Analysis**: 2-4 seconds
- **Document Analysis**: 3-6 seconds (depends on size)

### Optimization Tips
1. Use `llama-3.1-8b-instant` for faster responses
2. Lower `max_tokens` for quicker answers
3. Compress images before uploading
4. Use caching for repeated queries

---

## Security Features

1. **CORS Enabled**: Allows frontend on different port
2. **Rate Limiting**: Prevents abuse (60 req/min)
3. **File Size Limits**: Max 20MB to prevent DoS
4. **Input Validation**: Pydantic models validate all inputs
5. **Error Handling**: No stack traces exposed to users

---

## Monitoring

### Check Backend Logs
```bash
# Watch real-time logs
tail -f backend_logs.txt
```

### Check Active Connections
```bash
netstat -ano | findstr :8000
```

### Test Health
```bash
curl http://localhost:8000/health
```

---

## Deployment

### Local Development
```bash
cd backend
python working_vision_endpoint.py
```

### Production (with Gunicorn)
```bash
gunicorn -w 4 -k uvicorn.workers.UvicornWorker working_vision_endpoint:app
```

### Docker
```dockerfile
FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "working_vision_endpoint.py"]
```

---

## API Pricing

### Current Setup
- **Groq API**: FREE âœ…
- **Llama 3.3 70B**: FREE âœ…
- **All text models**: FREE âœ…
- **Image analysis (metadata)**: FREE âœ…
- **Hosting**: Your server

### Optional Upgrades
- **GPT-4 Vision**: ~$0.01-0.03 per image (OpenAI)
- **Claude Vision**: ~$0.025 per image (Anthropic)

**Your current setup costs: $0 for AI!** ðŸŽ‰

---

## Summary

### What We Use:
- **Framework**: FastAPI (Python)
- **AI Provider**: Groq (FREE)
- **Primary Model**: Llama 3.3 70B (70 billion parameters)
- **Image Processing**: Pillow + AI reasoning
- **Server**: Uvicorn ASGI

### Available Endpoints:
1. `GET /` - Server info
2. `GET /health` - Health check
3. `POST /chat` - Main chat + image analysis
4. `POST /upload` - File upload

### Key Features:
- âœ… 70B AI model (FREE!)
- âœ… Image metadata analysis
- âœ… Intelligent reasoning about images
- âœ… Document processing
- âœ… Rate limiting
- âœ… CORS enabled
- âœ… Full API documentation

**Everything is running and ready to use!** ðŸš€
